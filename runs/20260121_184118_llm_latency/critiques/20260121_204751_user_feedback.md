# User Feedback Log

## Overall direction
- Keep a consistent visual motif but less childish; still humorous/consistent.
- Increase technical correctness and precision across slides.
- Use consistent visual language for tokens, requests, batches, GPUs.

## Slide-specific feedback
- 00_cover: OK.
- 01_what_makes_slow: wording feels off; title should match exact desired wording.
- 02_metrics: remove p95. Show a full request as tokens; highlight prompt tokens as TTFT. Output tokens represent TBT * number of output tokens.
- 03_latency_budget: remove.
- 05_phases: combine with 02; use tokens-in-request visual instead of “phases” label.
- 06_output_dominates: needs a technical explanation for why output time dominates.
- 07_cadence_scatter: must be a real scatter plot with many dots; title should be “Trade-off Between Latency and Throughput.”
- 08_model_size: make the fast/slow relationship obvious and correct.
- 09_input_vs_output: currently unclear; should use token-request motif to show costs.
- 10_context_kv: unclear; needs a more technical depiction of context/KV growth.
- 11_cached_prefix: use token-request motif to show cached vs uncached prompt and output tokens.
- 12_long_requests: acceptable.
- 13_congestion: may need earlier placement.
- 14_batching: needs a technical depiction of stacking requests into a single forward pass.
- 15_routing: should show global data centers and network lines.
- 18_moe: needs explicit experts and skipping inactive experts.
- 19_proxy: show predicting a swatch of output tokens at once using the token motif.
- 20_quantize: more technical; show tokens with different numeric precision representations.
- 22_capacity: show adding capacity to an LLM serving engine.

## Additional guidance
- Use GPT‑5.2 if needed for higher technical fidelity in prompts and rubrics.
- Make graders more exacting and explicit.
